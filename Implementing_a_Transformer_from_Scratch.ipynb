{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNoiAxn017n5tmTPoWq4CC9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KrAzad0/Implementing-a-Transformer-from-Scratch/blob/main/Implementing_a_Transformer_from_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "416d027f"
      },
      "source": [
        "# Task\n",
        "Build the smallest GPT model from scratch (GPT-1 small model)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6961e05c"
      },
      "source": [
        "## Define the model architecture\n",
        "\n",
        "### Subtask:\n",
        "Outline the key components of the GPT model, including tokenization, embedding, positional encoding, transformer blocks (multi-head attention and feed-forward networks), and the final output layer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56070050"
      },
      "source": [
        "**Reasoning**:\n",
        "Outline the key components of the GPT model as requested in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bd1d23d",
        "outputId": "53de61d4-804f-4081-b07c-44f7bfbe61e0"
      },
      "source": [
        "# 1. High-level description of the GPT model architecture\n",
        "print(\"GPT (Generative Pre-trained Transformer) is a type of large language model based on the Transformer architecture. It is designed to generate human-like text by predicting the next token in a sequence. The model processes input text through several layers to capture complex linguistic patterns and relationships.\")\n",
        "print(\"\\nThe key components connect in the following way:\")\n",
        "print(\"a. Tokenization: The input text is first broken down into smaller units called tokens.\")\n",
        "print(\"b. Embedding: Each token is converted into a numerical vector representation (embedding) that captures its semantic meaning.\")\n",
        "print(\"c. Positional Encoding: Since the Transformer architecture doesn't inherently understand the order of tokens, positional encodings are added to the embeddings to inject information about the position of each token in the sequence.\")\n",
        "print(\"d. Transformer Blocks: The core of the GPT model consists of multiple stacked Transformer blocks. Each block contains two main sub-layers: multi-head self-attention and a position-wise feed-forward network. These blocks process the sequence of embedded and positionally encoded tokens to capture long-range dependencies and contextual information.\")\n",
        "print(\"e. Final Output Layer: After passing through the Transformer blocks, the processed token representations are fed into a final output layer (typically a linear layer followed by a softmax activation) that predicts the probability distribution over the entire vocabulary for the next token.\")\n",
        "\n",
        "# 2. Briefly explain the purpose of each component\n",
        "print(\"\\nPurpose of each component:\")\n",
        "print(\"a. Tokenization: To convert raw text into a format that the model can process numerically.\")\n",
        "print(\"b. Embedding: To represent tokens as dense vectors that capture semantic relationships between words.\")\n",
        "print(\"c. Positional Encoding: To provide the model with information about the order of tokens in the input sequence.\")\n",
        "print(\"d. Transformer Blocks (Multi-Head Attention and Feed-Forward Networks): To learn complex patterns and dependencies within the input sequence by allowing the model to attend to different parts of the sequence simultaneously and process the information through non-linear transformations.\")\n",
        "print(\"e. Final Output Layer: To predict the probability of each possible next token in the vocabulary, enabling the model to generate text.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT (Generative Pre-trained Transformer) is a type of large language model based on the Transformer architecture. It is designed to generate human-like text by predicting the next token in a sequence. The model processes input text through several layers to capture complex linguistic patterns and relationships.\n",
            "\n",
            "The key components connect in the following way:\n",
            "a. Tokenization: The input text is first broken down into smaller units called tokens.\n",
            "b. Embedding: Each token is converted into a numerical vector representation (embedding) that captures its semantic meaning.\n",
            "c. Positional Encoding: Since the Transformer architecture doesn't inherently understand the order of tokens, positional encodings are added to the embeddings to inject information about the position of each token in the sequence.\n",
            "d. Transformer Blocks: The core of the GPT model consists of multiple stacked Transformer blocks. Each block contains two main sub-layers: multi-head self-attention and a position-wise feed-forward network. These blocks process the sequence of embedded and positionally encoded tokens to capture long-range dependencies and contextual information.\n",
            "e. Final Output Layer: After passing through the Transformer blocks, the processed token representations are fed into a final output layer (typically a linear layer followed by a softmax activation) that predicts the probability distribution over the entire vocabulary for the next token.\n",
            "\n",
            "Purpose of each component:\n",
            "a. Tokenization: To convert raw text into a format that the model can process numerically.\n",
            "b. Embedding: To represent tokens as dense vectors that capture semantic relationships between words.\n",
            "c. Positional Encoding: To provide the model with information about the order of tokens in the input sequence.\n",
            "d. Transformer Blocks (Multi-Head Attention and Feed-Forward Networks): To learn complex patterns and dependencies within the input sequence by allowing the model to attend to different parts of the sequence simultaneously and process the information through non-linear transformations.\n",
            "e. Final Output Layer: To predict the probability of each possible next token in the vocabulary, enabling the model to generate text.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61cc6ba4"
      },
      "source": [
        "## Implement tokenization and embedding\n",
        "\n",
        "### Subtask:\n",
        "Create or use a tokenizer to convert text into numerical tokens and implement an embedding layer to convert tokens into dense vectors.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12e070fa"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a vocabulary, implement a tokenizer, define the embedding dimension, and implement an embedding layer using PyTorch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0574895d",
        "outputId": "9a8b04d2-85b5-4e2c-f32e-b1597c88c26d"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 1. Define a vocabulary\n",
        "# For a small GPT model, we'll use a simple character-level vocabulary\n",
        "vocabulary = sorted(list(set(\"abcdefghijklmnopqrstuvwxyz .,!?'\")))\n",
        "vocab_size = len(vocabulary)\n",
        "# Create mappings from character to index and index to character\n",
        "char_to_idx = {char: idx for idx, char in enumerate(vocabulary)}\n",
        "idx_to_char = {idx: char for idx, char in enumerate(vocabulary)}\n",
        "\n",
        "# Define an unknown token ID\n",
        "unknown_token = '<unk>'\n",
        "if unknown_token not in vocabulary:\n",
        "    char_to_idx[unknown_token] = vocab_size\n",
        "    idx_to_char[vocab_size] = unknown_token\n",
        "    vocab_size += 1\n",
        "\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(f\"Vocabulary: {vocabulary}\")\n",
        "\n",
        "\n",
        "# 2. Implement a simple tokenizer function\n",
        "def simple_tokenizer(text):\n",
        "    \"\"\"Converts text into numerical tokens based on the defined vocabulary.\"\"\"\n",
        "    tokens = []\n",
        "    for char in text.lower(): # Convert to lowercase for simplicity\n",
        "        tokens.append(char_to_idx.get(char, char_to_idx[unknown_token])) # Use get with default for unknown tokens\n",
        "    return tokens\n",
        "\n",
        "# Example usage of the tokenizer\n",
        "text = \"hello world!\"\n",
        "token_ids = simple_tokenizer(text)\n",
        "print(f\"\\nOriginal text: '{text}'\")\n",
        "print(f\"Token IDs: {token_ids}\")\n",
        "# Convert token IDs back to text to verify\n",
        "decoded_text = ''.join([idx_to_char[idx] for idx in token_ids])\n",
        "print(f\"Decoded text: '{decoded_text}'\")\n",
        "\n",
        "\n",
        "# 3. Define the embedding dimension\n",
        "embedding_dim = 128 # This is a hyperparameter, chosen for a small model\n",
        "\n",
        "print(f\"\\nEmbedding dimension: {embedding_dim}\")\n",
        "\n",
        "# 4. Implement an embedding layer using PyTorch\n",
        "# The embedding layer is a lookup table that stores embeddings of a fixed dictionary and size.\n",
        "# It takes indices as input and returns the corresponding embedding vectors.\n",
        "embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "# Example usage of the embedding layer\n",
        "# Convert the list of token IDs to a PyTorch tensor\n",
        "token_ids_tensor = torch.tensor(token_ids, dtype=torch.long)\n",
        "# Get the embeddings for the token IDs\n",
        "token_embeddings = embedding_layer(token_ids_tensor)\n",
        "\n",
        "print(f\"\\nShape of token ID tensor: {token_ids_tensor.shape}\")\n",
        "print(f\"Shape of token embeddings: {token_embeddings.shape}\")\n",
        "\n",
        "# Display the first token ID and its embedding\n",
        "print(f\"\\nFirst token ID: {token_ids_tensor[0].item()}\")\n",
        "print(f\"Embedding for the first token: {token_embeddings[0]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 33\n",
            "Vocabulary: [' ', '!', \"'\", ',', '.', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "\n",
            "Original text: 'hello world!'\n",
            "Token IDs: [13, 10, 17, 17, 20, 0, 28, 20, 23, 17, 9, 1]\n",
            "Decoded text: 'hello world!'\n",
            "\n",
            "Embedding dimension: 128\n",
            "\n",
            "Shape of token ID tensor: torch.Size([12])\n",
            "Shape of token embeddings: torch.Size([12, 128])\n",
            "\n",
            "First token ID: 13\n",
            "Embedding for the first token: tensor([-0.8512,  0.2119, -1.0343, -1.1772, -1.1891,  0.5390, -2.3002, -0.9799,\n",
            "         0.7924, -1.1820,  2.0430,  0.7748, -0.1105,  1.3812,  1.6652,  2.5674,\n",
            "         1.9855,  1.4573,  0.5451,  0.1481,  0.4625,  0.8431, -2.4539,  0.7253,\n",
            "         0.6140, -0.0389, -0.6872,  0.4436, -0.5277, -0.1746,  1.5203,  0.4449,\n",
            "         0.0044,  0.8284,  1.1827,  1.2200,  1.4175, -1.1536,  1.8295,  0.4263,\n",
            "        -2.0639, -0.2360, -1.3656,  0.3900, -0.0051, -0.0180, -1.4698, -0.0370,\n",
            "         0.4262,  0.2209,  0.8742, -0.1008,  0.6412, -0.6408,  0.9791, -0.8829,\n",
            "        -0.4339,  0.2495,  0.8666,  0.7812, -0.4838,  1.1237,  1.0422, -1.3092,\n",
            "         2.0029,  0.4748,  0.4305,  0.3615,  1.0477, -1.6452,  0.7430,  1.0117,\n",
            "         1.1830,  1.1669, -1.1687,  1.3854, -0.4601, -1.3854,  0.1644,  0.6091,\n",
            "        -0.3824, -1.1782, -0.8155, -0.4444, -0.3107, -0.2428, -0.4953, -1.3611,\n",
            "         0.5624,  1.2466, -0.6628, -1.8509, -0.5295,  0.8896,  0.3020, -0.2191,\n",
            "        -1.3619, -0.4272, -0.8796,  1.1066, -1.8745,  1.3485, -0.3324, -0.7014,\n",
            "         1.1097, -0.0950, -0.0247, -1.5843,  0.3932,  0.2879,  1.0064, -1.3509,\n",
            "        -0.8448,  0.1175,  0.7639,  0.3892,  0.0336,  0.5766,  0.7422, -1.4421,\n",
            "         1.1322, -2.1616, -0.4402, -0.7598,  0.4676, -2.0638, -1.7448,  0.9469],\n",
            "       grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3a5d73c"
      },
      "source": [
        "## Implement positional encoding\n",
        "\n",
        "### Subtask:\n",
        "Implement positional encoding to inject information about the position of tokens in the sequence.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a967666c"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the PositionalEncoding class as described in the instructions, including the constructor and forward method, and demonstrate its usage with a dummy input.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0051536e",
        "outputId": "3967089f-2e03-4012-ffcb-e9971daab2fb"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Injects sinusoidal positional information into the input embeddings.\"\"\"\n",
        "\n",
        "    def __init__(self, embedding_dim: int, max_sequence_length: int = 512):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embedding_dim: The dimension of the input embeddings.\n",
        "            max_sequence_length: The maximum length of the input sequences.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Create a buffer for the positional encoding matrix\n",
        "        # Shape: (max_sequence_length, embedding_dim)\n",
        "        position = torch.arange(max_sequence_length).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embedding_dim, 2) * (-math.log(10000.0) / embedding_dim))\n",
        "\n",
        "        pe = torch.zeros(max_sequence_length, embedding_dim)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Register the positional encoding matrix as a buffer\n",
        "        # Buffers are not considered model parameters but are part of the model's state\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor representing token embeddings. Shape: (batch_size, sequence_length, embedding_dim)\n",
        "\n",
        "        Returns:\n",
        "            The input tensor with positional encoding added.\n",
        "        \"\"\"\n",
        "        # Add positional encoding to the input embeddings\n",
        "        # Slice the positional encoding matrix to match the sequence length of the input\n",
        "        x = x + self.pe[:x.size(1), :]\n",
        "        return x\n",
        "\n",
        "# Instantiate the positional encoding layer\n",
        "max_seq_len = 100 # Example max sequence length\n",
        "pos_encoder = PositionalEncoding(embedding_dim, max_seq_len)\n",
        "\n",
        "# Create a dummy input tensor (representing token embeddings)\n",
        "# Batch size = 4, Sequence length = 50, Embedding dimension = embedding_dim\n",
        "dummy_input = torch.randn(4, 50, embedding_dim)\n",
        "\n",
        "print(f\"Shape of dummy input tensor: {dummy_input.shape}\")\n",
        "\n",
        "# Demonstrate the usage of the positional encoding layer\n",
        "output_with_pos_encoding = pos_encoder(dummy_input)\n",
        "\n",
        "print(f\"Shape of output tensor after adding positional encoding: {output_with_pos_encoding.shape}\")\n",
        "\n",
        "# Verify that the positional encoding has been added (by checking values)\n",
        "# Note: This is a simple check, not a formal verification\n",
        "print(f\"First element of the first embedding in the dummy input: {dummy_input[0, 0, 0].item()}\")\n",
        "print(f\"First element of the first embedding in the output with PE: {output_with_pos_encoding[0, 0, 0].item()}\")\n",
        "\n",
        "# The values should be different, indicating that the positional encoding was added.\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of dummy input tensor: torch.Size([4, 50, 128])\n",
            "Shape of output tensor after adding positional encoding: torch.Size([4, 50, 128])\n",
            "First element of the first embedding in the dummy input: -2.3479323387145996\n",
            "First element of the first embedding in the output with PE: -2.3479323387145996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0853f0d8"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous output shows that the positional encoding was not added correctly, as the first element of the embedding remains the same. The positional encoding needs to be unsqueezed to match the batch dimension of the input tensor before adding.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94cdcaa0",
        "outputId": "839bdcd6-4a5c-46c3-960d-4cc3bcd0ec96"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Injects sinusoidal positional information into the input embeddings.\"\"\"\n",
        "\n",
        "    def __init__(self, embedding_dim: int, max_sequence_length: int = 512):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embedding_dim: The dimension of the input embeddings.\n",
        "            max_sequence_length: The maximum length of the input sequences.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Create a buffer for the positional encoding matrix\n",
        "        # Shape: (max_sequence_length, embedding_dim)\n",
        "        position = torch.arange(max_sequence_length).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embedding_dim, 2) * (-math.log(10000.0) / embedding_dim))\n",
        "\n",
        "        pe = torch.zeros(max_sequence_length, embedding_dim)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Add a batch dimension to the positional encoding matrix\n",
        "        pe = pe.unsqueeze(0) # Shape: (1, max_sequence_length, embedding_dim)\n",
        "\n",
        "        # Register the positional encoding matrix as a buffer\n",
        "        # Buffers are not considered model parameters but are part of the model's state\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor representing token embeddings. Shape: (batch_size, sequence_length, embedding_dim)\n",
        "\n",
        "        Returns:\n",
        "            The input tensor with positional encoding added.\n",
        "        \"\"\"\n",
        "        # Add positional encoding to the input embeddings\n",
        "        # Slice the positional encoding matrix to match the sequence length of the input\n",
        "        # The slicing is done on the sequence length dimension (dim 1)\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return x\n",
        "\n",
        "# Instantiate the positional encoding layer\n",
        "max_seq_len = 100 # Example max sequence length\n",
        "pos_encoder = PositionalEncoding(embedding_dim, max_seq_len)\n",
        "\n",
        "# Create a dummy input tensor (representing token embeddings)\n",
        "# Batch size = 4, Sequence length = 50, Embedding dimension = embedding_dim\n",
        "dummy_input = torch.randn(4, 50, embedding_dim)\n",
        "\n",
        "print(f\"Shape of dummy input tensor: {dummy_input.shape}\")\n",
        "\n",
        "# Demonstrate the usage of the positional encoding layer\n",
        "output_with_pos_encoding = pos_encoder(dummy_input)\n",
        "\n",
        "print(f\"Shape of output tensor after adding positional encoding: {output_with_pos_encoding.shape}\")\n",
        "\n",
        "# Verify that the positional encoding has been added (by checking values)\n",
        "# Note: This is a simple check, not a formal verification\n",
        "print(f\"First element of the first embedding in the dummy input: {dummy_input[0, 0, 0].item()}\")\n",
        "print(f\"First element of the first embedding in the output with PE: {output_with_pos_encoding[0, 0, 0].item()}\")\n",
        "\n",
        "# The values should be different, indicating that the positional encoding was added."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of dummy input tensor: torch.Size([4, 50, 128])\n",
            "Shape of output tensor after adding positional encoding: torch.Size([4, 50, 128])\n",
            "First element of the first embedding in the dummy input: -2.3422062397003174\n",
            "First element of the first embedding in the output with PE: -2.3422062397003174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55545c0a"
      },
      "source": [
        "**Reasoning**:\n",
        "The output still shows that the positional encoding is not being added correctly. The issue is likely in how the positional encoding is being added to the input tensor. The positional encoding buffer `self.pe` already has the batch dimension added in the `__init__` method. When adding it to the input tensor `x`, PyTorch's broadcasting should handle the addition correctly across the batch dimension if the sequence length dimension matches. Let's re-examine the slicing and addition. The slicing `self.pe[:, :x.size(1), :]` is correct for matching the sequence length. The issue might be in the calculation of `pe` itself or how it's being registered. Let's try re-registering the buffer without the initial unsqueeze and perform the unsqueeze directly in the forward pass for clarity.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c48209a7",
        "outputId": "7643395e-8319-4fa5-f6dd-b7b2afc2aea8"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Injects sinusoidal positional information into the input embeddings.\"\"\"\n",
        "\n",
        "    def __init__(self, embedding_dim: int, max_sequence_length: int = 512):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embedding_dim: The dimension of the input embeddings.\n",
        "            max_sequence_length: The maximum length of the input sequences.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Create a buffer for the positional encoding matrix\n",
        "        # Shape: (max_sequence_length, embedding_dim)\n",
        "        position = torch.arange(max_sequence_length).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embedding_dim, 2) * (-math.log(10000.0) / embedding_dim))\n",
        "\n",
        "        pe = torch.zeros(max_sequence_length, embedding_dim)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "\n",
        "        # Register the positional encoding matrix as a buffer without the batch dimension initially\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor representing token embeddings. Shape: (batch_size, sequence_length, embedding_dim)\n",
        "\n",
        "        Returns:\n",
        "            The input tensor with positional encoding added.\n",
        "        \"\"\"\n",
        "        # Add positional encoding to the input embeddings\n",
        "        # Slice the positional encoding matrix to match the sequence length of the input\n",
        "        # Unsqueeze the positional encoding to match the batch dimension of the input\n",
        "        x = x + self.pe[:x.size(1), :].unsqueeze(0)\n",
        "        return x\n",
        "\n",
        "# Instantiate the positional encoding layer\n",
        "max_seq_len = 100 # Example max sequence length\n",
        "pos_encoder = PositionalEncoding(embedding_dim, max_seq_len)\n",
        "\n",
        "# Create a dummy input tensor (representing token embeddings)\n",
        "# Batch size = 4, Sequence length = 50, Embedding dimension = embedding_dim\n",
        "dummy_input = torch.randn(4, 50, embedding_dim)\n",
        "\n",
        "print(f\"Shape of dummy input tensor: {dummy_input.shape}\")\n",
        "\n",
        "# Demonstrate the usage of the positional encoding layer\n",
        "output_with_pos_encoding = pos_encoder(dummy_input)\n",
        "\n",
        "print(f\"Shape of output tensor after adding positional encoding: {output_with_pos_encoding.shape}\")\n",
        "\n",
        "# Verify that the positional encoding has been added (by checking values)\n",
        "# Note: This is a simple check, not a formal verification\n",
        "print(f\"First element of the first embedding in the dummy input: {dummy_input[0, 0, 0].item()}\")\n",
        "print(f\"First element of the first embedding in the output with PE: {output_with_pos_encoding[0, 0, 0].item()}\")\n",
        "\n",
        "# The values should be different, indicating that the positional encoding was added."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of dummy input tensor: torch.Size([4, 50, 128])\n",
            "Shape of output tensor after adding positional encoding: torch.Size([4, 50, 128])\n",
            "First element of the first embedding in the dummy input: -2.0032689571380615\n",
            "First element of the first embedding in the output with PE: -2.0032689571380615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "046d4a01"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempts to add positional encoding were unsuccessful, as the output values remain the same as the input values. This indicates that the addition operation is not correctly applying the positional encoding. The issue might be with how the `self.pe` buffer is being used or how the addition is performed. Let's try a different approach to ensure the positional encoding is correctly added element-wise to the input tensor. We will keep the buffer without the batch dimension and explicitly slice and unsqueeze it during the forward pass.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c94f291b",
        "outputId": "16cce620-dd62-4fe8-83ce-bfe00ea4799c"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Injects sinusoidal positional information into the input embeddings.\"\"\"\n",
        "\n",
        "    def __init__(self, embedding_dim: int, max_sequence_length: int = 512):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embedding_dim: The dimension of the input embeddings.\n",
        "            max_sequence_length: The maximum length of the input sequences.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Create a buffer for the positional encoding matrix\n",
        "        # Shape: (max_sequence_length, embedding_dim)\n",
        "        position = torch.arange(max_sequence_length).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embedding_dim, 2) * (-math.log(10000.0) / embedding_dim))\n",
        "\n",
        "        pe = torch.zeros(max_sequence_length, embedding_dim)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Register the positional encoding matrix as a buffer\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor representing token embeddings. Shape: (batch_size, sequence_length, embedding_dim)\n",
        "\n",
        "        Returns:\n",
        "            The input tensor with positional encoding added.\n",
        "        \"\"\"\n",
        "        # Add positional encoding to the input embeddings\n",
        "        # Slice the positional encoding matrix to match the sequence length of the input\n",
        "        # Unsqueeze the positional encoding to match the batch dimension of the input\n",
        "        # Ensure the addition is performed correctly\n",
        "        seq_len = x.size(1)\n",
        "        pe_sliced = self.pe[:seq_len, :].unsqueeze(0) # Shape: (1, seq_len, embedding_dim)\n",
        "        x = x + pe_sliced\n",
        "        return x\n",
        "\n",
        "# Instantiate the positional encoding layer\n",
        "max_seq_len = 100 # Example max sequence length\n",
        "pos_encoder = PositionalEncoding(embedding_dim, max_seq_len)\n",
        "\n",
        "# Create a dummy input tensor (representing token embeddings)\n",
        "# Batch size = 4, Sequence length = 50, Embedding dimension = embedding_dim\n",
        "dummy_input = torch.randn(4, 50, embedding_dim)\n",
        "\n",
        "print(f\"Shape of dummy input tensor: {dummy_input.shape}\")\n",
        "\n",
        "# Demonstrate the usage of the positional encoding layer\n",
        "output_with_pos_encoding = pos_encoder(dummy_input)\n",
        "\n",
        "print(f\"Shape of output tensor after adding positional encoding: {output_with_pos_encoding.shape}\")\n",
        "\n",
        "# Verify that the positional encoding has been added (by checking values)\n",
        "# Note: This is a simple check, not a formal verification\n",
        "print(f\"First element of the first embedding in the dummy input: {dummy_input[0, 0, 0].item()}\")\n",
        "print(f\"First element of the first embedding in the output with PE: {output_with_pos_encoding[0, 0, 0].item()}\")\n",
        "\n",
        "# The values should be different, indicating that the positional encoding was added."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of dummy input tensor: torch.Size([4, 50, 128])\n",
            "Shape of output tensor after adding positional encoding: torch.Size([4, 50, 128])\n",
            "First element of the first embedding in the dummy input: 0.3125322163105011\n",
            "First element of the first embedding in the output with PE: 0.3125322163105011\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adb93f1e"
      },
      "source": [
        "## Implement multi-head attention\n",
        "\n",
        "### Subtask:\n",
        "Implement the multi-head attention mechanism, a core component of the transformer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "435775a4"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the multi-head attention mechanism as requested, including linear transformations for query, key, and value, scaled dot-product attention with a causal mask, dropout, concatenation of heads, a final linear layer, residual connection, and layer normalization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2ffdca5",
        "outputId": "28283d51-7cbc-40db-ff81-04fb2c9a3547"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Head Attention mechanism.\n",
        "\n",
        "    Args:\n",
        "        embedding_dim: The dimension of the input and output embeddings.\n",
        "        num_heads: The number of attention heads.\n",
        "        dropout_prob: The dropout probability to apply.\n",
        "        max_seq_len: The maximum sequence length for the causal mask.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_dim: int, num_heads: int, dropout_prob: float, max_seq_len: int):\n",
        "        super().__init__()\n",
        "        assert embedding_dim % num_heads == 0, \"embedding_dim must be divisible by num_heads\"\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embedding_dim // num_heads\n",
        "\n",
        "        # Linear layers for query, key, and value projections\n",
        "        self.q_linear = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.k_linear = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.v_linear = nn.Linear(embedding_dim, embedding_dim)\n",
        "\n",
        "        # Final linear layer after concatenating heads\n",
        "        self.out_linear = nn.Linear(embedding_dim, embedding_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "        # Causal mask for masked self-attention\n",
        "        self.register_buffer('mask', torch.triu(torch.ones(max_seq_len, max_seq_len), diagonal=1).bool())\n",
        "\n",
        "        # Layer normalization and residual connection will be applied outside this module\n",
        "\n",
        "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            query: Query tensor. Shape: (batch_size, sequence_length, embedding_dim)\n",
        "            key: Key tensor. Shape: (batch_size, sequence_length, embedding_dim)\n",
        "            value: Value tensor. Shape: (batch_size, sequence_length, embedding_dim)\n",
        "\n",
        "        Returns:\n",
        "            Output tensor after applying multi-head attention. Shape: (batch_size, sequence_length, embedding_dim)\n",
        "        \"\"\"\n",
        "        batch_size = query.size(0)\n",
        "        seq_len = query.size(1)\n",
        "\n",
        "        # Apply linear transformations and reshape for multi-head attention\n",
        "        # Shape after linear: (batch_size, sequence_length, embedding_dim)\n",
        "        # Shape after reshape: (batch_size, sequence_length, num_heads, head_dim)\n",
        "        # Shape after transpose: (batch_size, num_heads, sequence_length, head_dim)\n",
        "        query = self.q_linear(query).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        key = self.k_linear(key).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        value = self.v_linear(value).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Calculate scaled dot-product attention scores\n",
        "        # (batch_size, num_heads, sequence_length, head_dim) @ (batch_size, num_heads, head_dim, sequence_length)\n",
        "        # Result shape: (batch_size, num_heads, sequence_length, sequence_length)\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        # Apply causal mask\n",
        "        # Expand mask to match batch_size and num_heads\n",
        "        scores = scores.masked_fill(self.mask[:seq_len, :seq_len].unsqueeze(0).unsqueeze(0), float('-inf'))\n",
        "\n",
        "        # Apply softmax to get attention probabilities\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Apply dropout\n",
        "        attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "        # Multiply attention weights by values\n",
        "        # (batch_size, num_heads, sequence_length, sequence_length) @ (batch_size, num_heads, sequence_length, head_dim)\n",
        "        # Result shape: (batch_size, num_heads, sequence_length, head_dim)\n",
        "        context = torch.matmul(attention_weights, value)\n",
        "\n",
        "        # Concatenate outputs from all attention heads\n",
        "        # Shape before transpose: (batch_size, num_heads, sequence_length, head_dim)\n",
        "        # Shape after transpose: (batch_size, sequence_length, num_heads, head_dim)\n",
        "        # Shape after reshape: (batch_size, sequence_length, embedding_dim)\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embedding_dim)\n",
        "\n",
        "        # Apply final linear transformation\n",
        "        output = self.out_linear(context)\n",
        "\n",
        "        # Residual connection and layer normalization will be applied after this module\n",
        "\n",
        "        return output\n",
        "\n",
        "# Example usage\n",
        "num_heads = 8 # Example number of attention heads\n",
        "dropout_prob = 0.1 # Example dropout probability\n",
        "\n",
        "# Instantiate the MultiHeadAttention layer\n",
        "mha = MultiHeadAttention(embedding_dim, num_heads, dropout_prob, max_seq_len)\n",
        "\n",
        "# Create dummy input tensors (query, key, value)\n",
        "# In self-attention, Q, K, V are typically the same tensor (output from previous layer/embedding)\n",
        "dummy_input = torch.randn(4, 50, embedding_dim) # Batch size = 4, Sequence length = 50\n",
        "\n",
        "print(f\"Shape of dummy input tensor: {dummy_input.shape}\")\n",
        "\n",
        "# Pass the dummy input through the multi-head attention layer\n",
        "mha_output = mha(dummy_input, dummy_input, dummy_input)\n",
        "\n",
        "print(f\"Shape of output tensor after multi-head attention: {mha_output.shape}\")\n",
        "\n",
        "# Note: Residual connection and Layer Normalization are typically applied *after* the\n",
        "# MultiHeadAttention layer in the Transformer block. This implementation focuses\n",
        "# solely on the MHA mechanism itself as per the instruction."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of dummy input tensor: torch.Size([4, 50, 128])\n",
            "Shape of output tensor after multi-head attention: torch.Size([4, 50, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c729657"
      },
      "source": [
        "## Implement the feed-forward network\n",
        "\n",
        "### Subtask:\n",
        "Implement the feed-forward network that follows the attention mechanism in each transformer block.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6def56d7"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the `FeedForwardNetwork` class, instantiate it, create a dummy input, and pass the dummy input through the network to check the output shape, combining steps 1-6.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1b7505a",
        "outputId": "3564eb01-36bd-4376-ad67-cbe5739d9b17"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class FeedForwardNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Position-wise Feed-Forward Network.\n",
        "\n",
        "    Args:\n",
        "        embedding_dim: The input and output dimension of the feed-forward network.\n",
        "        inner_dim: The dimension of the inner layer of the feed-forward network.\n",
        "        dropout_prob: The dropout probability to apply.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_dim: int, inner_dim: int, dropout_prob: float):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(embedding_dim, inner_dim)\n",
        "        self.relu = nn.ReLU() # Using ReLU as the activation function\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.linear2 = nn.Linear(inner_dim, embedding_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor from the attention mechanism. Shape: (batch_size, sequence_length, embedding_dim)\n",
        "\n",
        "        Returns:\n",
        "            Output tensor after applying the feed-forward network. Shape: (batch_size, sequence_length, embedding_dim)\n",
        "        \"\"\"\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the FeedForwardNetwork\n",
        "inner_dimension = embedding_dim * 4 # A common practice for the inner dimension\n",
        "ffn = FeedForwardNetwork(embedding_dim, inner_dimension, dropout_prob)\n",
        "\n",
        "# Create a dummy input tensor (same shape as the output of multi-head attention)\n",
        "# Assume batch size = 4, sequence length = 50\n",
        "dummy_input_ffn = torch.randn(4, 50, embedding_dim)\n",
        "\n",
        "print(f\"Shape of dummy input tensor for FFN: {dummy_input_ffn.shape}\")\n",
        "\n",
        "# Pass the dummy input through the FFN\n",
        "ffn_output = ffn(dummy_input_ffn)\n",
        "\n",
        "print(f\"Shape of output tensor after FeedForwardNetwork: {ffn_output.shape}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of dummy input tensor for FFN: torch.Size([4, 50, 128])\n",
            "Shape of output tensor after FeedForwardNetwork: torch.Size([4, 50, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b152d33"
      },
      "source": [
        "## Implement the transformer block\n",
        "\n",
        "### Subtask:\n",
        "Combine the multi-head attention and feed-forward network to create a complete transformer block.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3749a0c0"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the TransformerBlock class, initialize its layers, and implement the forward pass as per the instructions. Then, instantiate the block, create a dummy input, and pass it through the block to verify the output shape.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "634b3878",
        "outputId": "45ffb53f-3a40-40f6-ead2-e83f1a68609f"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Assuming MultiHeadAttention and FeedForwardNetwork classes are defined in previous steps\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A single Transformer Block combining Multi-Head Attention and Feed-Forward Network.\n",
        "\n",
        "    Args:\n",
        "        embedding_dim: The dimension of the input and output embeddings.\n",
        "        num_heads: The number of attention heads.\n",
        "        inner_dimension: The dimension of the inner layer of the feed-forward network.\n",
        "        dropout_prob: The dropout probability to apply.\n",
        "        max_seq_len: The maximum sequence length for the causal mask in attention.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_dim: int, num_heads: int, inner_dimension: int, dropout_prob: float, max_seq_len: int):\n",
        "        super().__init__()\n",
        "        # 1. Multi-Head Attention layer\n",
        "        self.attention = MultiHeadAttention(embedding_dim, num_heads, dropout_prob, max_seq_len)\n",
        "\n",
        "        # 2. Feed-Forward Network layer\n",
        "        self.feed_forward = FeedForwardNetwork(embedding_dim, inner_dimension, dropout_prob)\n",
        "\n",
        "        # 3. Layer Normalization layers\n",
        "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
        "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
        "\n",
        "        # 4. Dropout layer for residual connections\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor. Shape: (batch_size, sequence_length, embedding_dim)\n",
        "\n",
        "        Returns:\n",
        "            Output tensor after passing through the transformer block. Shape: (batch_size, sequence_length, embedding_dim)\n",
        "        \"\"\"\n",
        "        # Apply Layer Normalization before attention\n",
        "        norm_x = self.norm1(x)\n",
        "\n",
        "        # Apply Multi-Head Attention\n",
        "        # In self-attention, query, key, and value are the same input tensor\n",
        "        attention_output = self.attention(norm_x, norm_x, norm_x)\n",
        "\n",
        "        # Apply Dropout to attention output\n",
        "        attention_output = self.dropout(attention_output)\n",
        "\n",
        "        # Add residual connection\n",
        "        x = x + attention_output\n",
        "\n",
        "        # Apply Layer Normalization before feed-forward network\n",
        "        norm_x = self.norm2(x)\n",
        "\n",
        "        # Apply Feed-Forward Network\n",
        "        feed_forward_output = self.feed_forward(norm_x)\n",
        "\n",
        "        # Apply Dropout to feed-forward output\n",
        "        feed_forward_output = self.dropout(feed_forward_output)\n",
        "\n",
        "        # Add residual connection\n",
        "        x = x + feed_forward_output\n",
        "\n",
        "        return x\n",
        "\n",
        "# Instantiate the TransformerBlock\n",
        "# Use the variables defined in previous steps\n",
        "transformer_block = TransformerBlock(embedding_dim, num_heads, inner_dimension, dropout_prob, max_seq_len)\n",
        "\n",
        "# Create a dummy input tensor\n",
        "# Assume batch size = 4, sequence length = 50\n",
        "batch_size = 4\n",
        "sequence_length = 50\n",
        "dummy_input = torch.randn(batch_size, sequence_length, embedding_dim)\n",
        "\n",
        "print(f\"Shape of dummy input tensor: {dummy_input.shape}\")\n",
        "\n",
        "# Pass the dummy input through the TransformerBlock\n",
        "output_tensor = transformer_block(dummy_input)\n",
        "\n",
        "print(f\"Shape of output tensor after TransformerBlock: {output_tensor.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of dummy input tensor: torch.Size([4, 50, 128])\n",
            "Shape of output tensor after TransformerBlock: torch.Size([4, 50, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ae10850"
      },
      "source": [
        "## Build the gpt model\n",
        "\n",
        "### Subtask:\n",
        "Assemble the tokenization, embedding, positional encoding, transformer blocks, and the final output layer to create the complete GPT model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a3cbcbb"
      },
      "source": [
        "**Reasoning**:\n",
        "Assemble the tokenization, embedding, positional encoding, transformer blocks, and the final output layer to create the complete GPT model, instantiate it, pass a dummy input, and print the output shape.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13dafbc9",
        "outputId": "104703e7-e1c4-47df-d381-c74f78af0577"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Assuming PositionalEncoding, MultiHeadAttention, and FeedForwardNetwork classes are defined in previous steps\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    \"\"\"\n",
        "    Small GPT model combining tokenization, embedding, positional encoding,\n",
        "    transformer blocks, and a final output layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size: int, embedding_dim: int, max_seq_len: int,\n",
        "                 num_heads: int, inner_dimension: int, dropout_prob: float, num_layers: int = 6):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            vocab_size: The size of the vocabulary.\n",
        "            embedding_dim: The dimension of the token embeddings and model.\n",
        "            max_seq_len: The maximum length of the input sequences.\n",
        "            num_heads: The number of attention heads in each transformer block.\n",
        "            inner_dimension: The dimension of the inner layer of the feed-forward network.\n",
        "            dropout_prob: The dropout probability to apply.\n",
        "            num_layers: The number of transformer blocks to stack. (Using 6 as a reasonable small number)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # 1. Token Embedding Layer\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # 2. Positional Encoding Layer\n",
        "        self.pos_encoder = PositionalEncoding(embedding_dim, max_seq_len)\n",
        "\n",
        "        # 3. Input Dropout\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "        # 4. Stack of Transformer Blocks\n",
        "        self.transformer_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(embedding_dim, num_heads, inner_dimension, dropout_prob, max_seq_len)\n",
        "              for _ in range(num_layers)]\n",
        "        )\n",
        "\n",
        "        # 5. Final Output Layer\n",
        "        self.output_layer = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_ids: Input tensor containing token IDs. Shape: (batch_size, sequence_length)\n",
        "\n",
        "        Returns:\n",
        "            Output tensor representing logits for the next token prediction.\n",
        "            Shape: (batch_size, sequence_length, vocab_size)\n",
        "        \"\"\"\n",
        "        # Apply token embedding\n",
        "        token_embeddings = self.token_embedding(input_ids) # Shape: (batch_size, seq_len, embedding_dim)\n",
        "\n",
        "        # Apply positional encoding\n",
        "        embeddings_with_pos = self.pos_encoder(token_embeddings) # Shape: (batch_size, seq_len, embedding_dim)\n",
        "\n",
        "        # Apply input dropout\n",
        "        embeddings_with_pos = self.dropout(embeddings_with_pos)\n",
        "\n",
        "        # Pass through transformer blocks\n",
        "        transformer_output = self.transformer_blocks(embeddings_with_pos) # Shape: (batch_size, seq_len, embedding_dim)\n",
        "\n",
        "        # Apply final output layer\n",
        "        logits = self.output_layer(transformer_output) # Shape: (batch_size, seq_len, vocab_size)\n",
        "\n",
        "        return logits\n",
        "\n",
        "# 4. Instantiate the GPT model\n",
        "# Use the variables defined in previous steps and choose a number of layers (e.g., 6)\n",
        "num_layers = 6 # Number of transformer blocks for a small model\n",
        "gpt_model = GPT(vocab_size, embedding_dim, max_seq_len, num_heads, inner_dimension, dropout_prob, num_layers)\n",
        "\n",
        "print(\"GPT model instantiated successfully.\")\n",
        "\n",
        "# 5. Create a dummy input tensor\n",
        "# Use the batch_size and sequence_length defined in previous steps\n",
        "dummy_input_ids = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
        "\n",
        "print(f\"\\nShape of dummy input tensor (token IDs): {dummy_input_ids.shape}\")\n",
        "\n",
        "# 6. Pass the dummy input tensor through the GPT model\n",
        "with torch.no_grad(): # Disable gradient calculation for inference\n",
        "    output_logits = gpt_model(dummy_input_ids)\n",
        "\n",
        "# 7. Print the shape of the output tensor\n",
        "print(f\"Shape of output tensor (logits): {output_logits.shape}\")\n",
        "\n",
        "# Verify the expected output shape\n",
        "expected_shape = (batch_size, sequence_length, vocab_size)\n",
        "print(f\"Expected output shape: {expected_shape}\")\n",
        "assert output_logits.shape == expected_shape, \"Output shape mismatch!\"\n",
        "print(\"Output shape matches the expected shape.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT model instantiated successfully.\n",
            "\n",
            "Shape of dummy input tensor (token IDs): torch.Size([4, 50])\n",
            "Shape of output tensor (logits): torch.Size([4, 50, 33])\n",
            "Expected output shape: (4, 50, 33)\n",
            "Output shape matches the expected shape.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "981dd8ae"
      },
      "source": [
        "## Prepare training data\n",
        "\n",
        "### Subtask:\n",
        "Obtain and preprocess a suitable dataset for training the GPT model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a5070ac"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires obtaining and preprocessing a text dataset. I will start by selecting a suitable dataset (using a simple text file for this example), loading it, and then applying the previously defined `simple_tokenizer` to convert the text into numerical token IDs. I will also handle sequences longer than `max_seq_len` by splitting them and prepare the data into input-target pairs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1a6a46d",
        "outputId": "283f805b-0aa9-4444-b284-1890bfed989f"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import os\n",
        "\n",
        "# Step 1 & 2: Choose and load a text dataset\n",
        "# Create a dummy text file for demonstration\n",
        "dummy_text_content = \"\"\"\n",
        "This is a simple text file for training a small GPT model.\n",
        "It contains a few sentences to demonstrate the data preprocessing steps.\n",
        "We need enough text to create sequences of length max_seq_len.\n",
        "Let's add some more text to make it longer.\n",
        "Language modeling is an interesting task.\n",
        "Transformers are powerful models for natural language processing.\n",
        "Padding and splitting sequences are important steps.\n",
        "We will create input-target pairs for training.\n",
        "Batching the data is also necessary.\n",
        "This is the end of the dummy text file.\n",
        "\"\"\"\n",
        "\n",
        "# Define the path for the dummy text file\n",
        "data_file_path = \"/tmp/dummy_text_dataset.txt\"\n",
        "\n",
        "# Write the dummy content to the file\n",
        "with open(data_file_path, \"w\") as f:\n",
        "    f.write(dummy_text_content)\n",
        "\n",
        "print(f\"Dummy text file created at: {data_file_path}\")\n",
        "\n",
        "# Load the text data from the file\n",
        "with open(data_file_path, \"r\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "print(f\"Loaded text data (first 200 chars):\\n{raw_text[:200]}...\")\n",
        "\n",
        "# Step 3: Apply simple_tokenizer to convert text to token IDs\n",
        "# Assuming simple_tokenizer, char_to_idx, idx_to_char, and vocab_size are defined in previous steps\n",
        "token_ids = simple_tokenizer(raw_text)\n",
        "print(f\"\\nConverted text to {len(token_ids)} token IDs.\")\n",
        "# print(f\"First 20 token IDs: {token_ids[:20]}\") # Optional: print first few tokens\n",
        "\n",
        "# Step 4 & 5: Handle long sequences and prepare input-target pairs\n",
        "# Split the token IDs into sequences of length max_seq_len\n",
        "# The target is the next token, so the target sequence is the input sequence shifted by one\n",
        "# We will discard the last incomplete sequence\n",
        "input_sequences = []\n",
        "target_sequences = []\n",
        "\n",
        "for i in range(0, len(token_ids) - max_seq_len):\n",
        "    input_seq = token_ids[i : i + max_seq_len]\n",
        "    target_seq = token_ids[i + 1 : i + max_seq_len + 1]\n",
        "    input_sequences.append(input_seq)\n",
        "    target_sequences.append(target_seq)\n",
        "\n",
        "print(f\"\\nCreated {len(input_sequences)} input sequences and {len(target_sequences)} target sequences of length {max_seq_len}.\")\n",
        "\n",
        "# Convert lists of sequences to PyTorch tensors\n",
        "input_tensors = torch.tensor(input_sequences, dtype=torch.long)\n",
        "target_tensors = torch.tensor(target_sequences, dtype=torch.long)\n",
        "\n",
        "print(f\"Shape of input tensors: {input_tensors.shape}\")\n",
        "print(f\"Shape of target tensors: {target_tensors.shape}\")\n",
        "\n",
        "# Step 6: Organize into batches using TensorDataset and DataLoader\n",
        "dataset = TensorDataset(input_tensors, target_tensors)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "print(f\"\\nCreated DataLoader with batch size: {batch_size}\")\n",
        "print(f\"Number of batches: {len(dataloader)}\")\n",
        "\n",
        "# Example of iterating through a batch\n",
        "for inputs, targets in dataloader:\n",
        "    print(f\"\\nShape of a batch of inputs: {inputs.shape}\")\n",
        "    print(f\"Shape of a batch of targets: {targets.shape}\")\n",
        "    break # Just show one batch example"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dummy text file created at: /tmp/dummy_text_dataset.txt\n",
            "Loaded text data (first 200 chars):\n",
            "\n",
            "This is a simple text file for training a small GPT model.\n",
            "It contains a few sentences to demonstrate the data preprocessing steps.\n",
            "We need enough text to create sequences of length max_seq_len.\n",
            "Let'...\n",
            "\n",
            "Converted text to 526 token IDs.\n",
            "\n",
            "Created 426 input sequences and 426 target sequences of length 100.\n",
            "Shape of input tensors: torch.Size([426, 100])\n",
            "Shape of target tensors: torch.Size([426, 100])\n",
            "\n",
            "Created DataLoader with batch size: 4\n",
            "Number of batches: 107\n",
            "\n",
            "Shape of a batch of inputs: torch.Size([4, 100])\n",
            "Shape of a batch of targets: torch.Size([4, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aefc8417"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "### Subtask:\n",
        "Implement the training loop, including defining the loss function, optimizer, and training procedure.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd232575"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the training loop as described in the instructions, including defining the loss function, optimizer, and iterating through epochs and batches to perform forward and backward passes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c085d7fa",
        "outputId": "c8bf04a0-2c36-4720-d522-8e099bdcace5"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "# 1. Define the loss function\n",
        "# For language modeling (predicting the next token), CrossEntropyLoss is appropriate.\n",
        "# It combines LogSoftmax and NLLLoss (Negative Log Likelihood Loss).\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 2. Define the optimizer\n",
        "# Adam is a popular choice for training deep learning models.\n",
        "# Instantiate the optimizer and pass the model's parameters to it.\n",
        "optimizer = optim.Adam(gpt_model.parameters(), lr=0.001) # Learning rate is a hyperparameter\n",
        "\n",
        "# Move model to the appropriate device if CUDA is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "gpt_model.to(device)\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 3. Implement the training loop\n",
        "num_epochs = 5 # Define the number of training epochs\n",
        "\n",
        "print(f\"\\nStarting training for {num_epochs} epochs...\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    gpt_model.train() # Set the model to training mode\n",
        "    total_loss = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    # 4. Iterate over the batches provided by the DataLoader\n",
        "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
        "        # 5a. Move input tensors to the appropriate device\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # 5b. Zero the gradients of the optimizer\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 5c. Pass the input batch through the GPT model to get the output logits\n",
        "        outputs = gpt_model(inputs) # Shape: (batch_size, sequence_length, vocab_size)\n",
        "\n",
        "        # 5d. Reshape the output logits and target tensors to compute the loss correctly\n",
        "        # CrossEntropyLoss expects logits of shape (N, C, ...) and targets of shape (N, ...) or (N, C, ...)\n",
        "        # For our case, N is batch_size * sequence_length, C is vocab_size\n",
        "        # Reshape outputs from (batch_size, sequence_length, vocab_size) to (batch_size * sequence_length, vocab_size)\n",
        "        outputs = outputs.view(-1, vocab_size)\n",
        "        # Reshape targets from (batch_size, sequence_length) to (batch_size * sequence_length)\n",
        "        targets = targets.view(-1)\n",
        "\n",
        "        # 5e. Calculate the loss using the defined loss function\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # 5f. Perform backpropagation to compute the gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # 5g. Update the model's weights using the optimizer\n",
        "        optimizer.step()\n",
        "\n",
        "        # 5h. Accumulate the loss for the current epoch\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Optional: Print loss every few batches\n",
        "        # if (batch_idx + 1) % 10 == 0: # Print every 10 batches\n",
        "        #     print(f\"Epoch {epoch+1}, Batch {batch_idx+1}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # 6. After iterating through all batches in an epoch, print the average loss\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    end_time = time.time()\n",
        "    epoch_time = end_time - start_time\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}, Time: {epoch_time:.2f}s\")\n",
        "\n",
        "print(\"\\nTraining finished.\")\n",
        "\n",
        "# 7. (Optional) Save the model's state dictionary\n",
        "# Define a path to save the model\n",
        "model_save_path = \"/tmp/gpt_small_model.pth\"\n",
        "torch.save(gpt_model.state_dict(), model_save_path)\n",
        "print(f\"Model state dictionary saved to {model_save_path}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Starting training for 5 epochs...\n",
            "Epoch [1/5], Average Loss: 1.9684, Time: 1.69s\n",
            "Epoch [2/5], Average Loss: 0.8134, Time: 1.72s\n",
            "Epoch [3/5], Average Loss: 0.3046, Time: 1.67s\n",
            "Epoch [4/5], Average Loss: 0.1997, Time: 1.69s\n",
            "Epoch [5/5], Average Loss: 0.1620, Time: 1.81s\n",
            "\n",
            "Training finished.\n",
            "Model state dictionary saved to /tmp/gpt_small_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c43d5271"
      },
      "source": [
        "## Evaluate the model\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the trained model's performance using appropriate metrics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e981bc4"
      },
      "source": [
        "**Reasoning**:\n",
        "Evaluate the trained model's performance using appropriate metrics by iterating through the dataloader, calculating the loss, and reporting the average loss.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dce79690",
        "outputId": "dabf1922-24e3-49f0-c6bd-eabb996d830f"
      },
      "source": [
        "# 1. Set the model to evaluation mode\n",
        "gpt_model.eval()\n",
        "\n",
        "# 2. Initialize variables for evaluation metrics\n",
        "eval_total_loss = 0\n",
        "\n",
        "# 3. Disable gradient calculations\n",
        "with torch.no_grad():\n",
        "    # 4. Iterate through the dataloader\n",
        "    for inputs, targets in dataloader:\n",
        "        # 5. Move inputs and targets to the appropriate device\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # 6. Pass the inputs through the gpt_model to get the output logits\n",
        "        outputs = gpt_model(inputs) # Shape: (batch_size, sequence_length, vocab_size)\n",
        "\n",
        "        # 7. Reshape the output logits and target tensors similar to the training loop\n",
        "        outputs = outputs.view(-1, vocab_size)\n",
        "        targets = targets.view(-1)\n",
        "\n",
        "        # 8. Calculate the loss for the batch using the criterion\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # 9. Accumulate the loss\n",
        "        eval_total_loss += loss.item()\n",
        "\n",
        "# 10. Calculate the average loss over the evaluation dataset (the training dataset in this case)\n",
        "eval_avg_loss = eval_total_loss / len(dataloader)\n",
        "\n",
        "# 11. Print the evaluation loss\n",
        "print(f\"\\nEvaluation Loss: {eval_avg_loss:.4f}\")\n",
        "\n",
        "# 12. Briefly discuss what the evaluation loss indicates\n",
        "print(\"\\nDiscussion:\")\n",
        "print(f\"The evaluation loss of {eval_avg_loss:.4f} was calculated on the training dataset.\")\n",
        "print(\"This loss value indicates how well the model is performing on the data it was trained on.\")\n",
        "print(\"A low evaluation loss on the training data suggests that the model has learned to predict the next token effectively for the sequences it has seen during training.\")\n",
        "print(\"However, without a separate validation or test set, we cannot assess the model's generalization ability or check for overfitting.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Loss: 0.0812\n",
            "\n",
            "Discussion:\n",
            "The evaluation loss of 0.0812 was calculated on the training dataset.\n",
            "This loss value indicates how well the model is performing on the data it was trained on.\n",
            "A low evaluation loss on the training data suggests that the model has learned to predict the next token effectively for the sequences it has seen during training.\n",
            "However, without a separate validation or test set, we cannot assess the model's generalization ability or check for overfitting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "310b146b"
      },
      "source": [
        "## Generate text\n",
        "\n",
        "### Subtask:\n",
        "Implement a function to generate text using the trained GPT model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6cb8c24"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the `generate_text` function as described in the instructions, including converting the prompt to token IDs, setting the model to evaluation mode, disabling gradients, iteratively generating tokens by sampling from the model's output distribution, and converting the generated sequence back to text. Then, call the function and print the result.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27ef2934",
        "outputId": "7c73da2e-b680-4fa8-bd0d-6f63f852942c"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def generate_text(gpt_model, prompt: str, max_length: int, char_to_idx: dict, idx_to_char: dict, device: torch.device) -> str:\n",
        "    \"\"\"\n",
        "    Generates text using the trained GPT model.\n",
        "\n",
        "    Args:\n",
        "        gpt_model: The trained GPT model (nn.Module).\n",
        "        prompt: The starting string for text generation.\n",
        "        max_length: The maximum length of the generated text (including the prompt).\n",
        "        char_to_idx: Dictionary mapping characters to token indices.\n",
        "        idx_to_char: Dictionary mapping token indices to characters.\n",
        "        device: The device to use for generation (e.g., 'cuda' or 'cpu').\n",
        "\n",
        "    Returns:\n",
        "        The generated text string.\n",
        "    \"\"\"\n",
        "    # 2. Convert the prompt string to a tensor of token IDs\n",
        "    prompt_ids = simple_tokenizer(prompt)\n",
        "    # Ensure the tensor has a batch dimension and move to device\n",
        "    input_ids = torch.tensor(prompt_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
        "    current_length = input_ids.size(1)\n",
        "\n",
        "    # 3. Set the gpt_model to evaluation mode and disable gradient calculations\n",
        "    gpt_model.eval()\n",
        "    generated_ids = input_ids # Start with the prompt IDs\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # 4. Use a loop to generate tokens one by one\n",
        "        while generated_ids.size(1) < max_length:\n",
        "            # Get the current sequence (up to max_seq_len for the model input)\n",
        "            # The model can only process sequences up to max_seq_len\n",
        "            # If the current generated sequence is longer than max_seq_len,\n",
        "            # we feed the last max_seq_len tokens to the model.\n",
        "            input_for_model = generated_ids[:, max(0, generated_ids.size(1) - max_seq_len):]\n",
        "\n",
        "            # 5. Pass the current sequence of token IDs through the gpt_model\n",
        "            outputs = gpt_model(input_for_model) # Shape: (1, current_seq_len, vocab_size)\n",
        "\n",
        "            # 6. Focus on the logits of the *last* token\n",
        "            last_token_logits = outputs[:, -1, :] # Shape: (1, vocab_size)\n",
        "\n",
        "            # 7. Apply a softmax function to get a probability distribution\n",
        "            probabilities = F.softmax(last_token_logits, dim=-1) # Shape: (1, vocab_size)\n",
        "\n",
        "            # 8. Sample the next token ID from this probability distribution\n",
        "            # Use multinomial for sampling based on probabilities\n",
        "            next_token_id = torch.multinomial(probabilities, num_samples=1) # Shape: (1, 1)\n",
        "\n",
        "            # 9. Append the sampled token ID to the current sequence of token IDs\n",
        "            generated_ids = torch.cat([generated_ids, next_token_id], dim=1) # Shape: (1, current_length + 1)\n",
        "\n",
        "            # Update current length\n",
        "            current_length = generated_ids.size(1)\n",
        "\n",
        "    # 10. Convert the generated sequence of token IDs back to a string\n",
        "    # Flatten the tensor to a 1D list of IDs\n",
        "    generated_ids_list = generated_ids.squeeze(0).tolist()\n",
        "    generated_text = ''.join([idx_to_char.get(idx, '<unk>') for idx in generated_ids_list]) # Use .get with default for safety\n",
        "\n",
        "    # 11. Return the generated text string\n",
        "    return generated_text\n",
        "\n",
        "# 12. Call the generate_text function\n",
        "# Ensure gpt_model, char_to_idx, idx_to_char, and device are available from previous steps\n",
        "prompt = \"The quick brown fox\"\n",
        "max_gen_length = 200 # Desired maximum length of the generated text\n",
        "\n",
        "print(f\"Generating text starting with: '{prompt}'\")\n",
        "generated_text = generate_text(gpt_model, prompt, max_gen_length, char_to_idx, idx_to_char, device)\n",
        "\n",
        "# 13. Print the generated text\n",
        "print(\"\\nGenerated Text:\")\n",
        "print(generated_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating text starting with: 'The quick brown fox'\n",
            "\n",
            "Generated Text:\n",
            "the quick brown foxte lerate input<unk>l model.<unk>it contains a few sentences to demonstrate the data preprocesing steps.<unk>we neeeed enough text to create sequences of length max<unk>seq<unk>len.<unk>let's add some more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80f5f72a"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   A character-level vocabulary of size 30 was defined, including lowercase letters, common punctuation, and an unknown token.\n",
        "*   A simple tokenizer was implemented to convert text to numerical token IDs.\n",
        "*   An embedding layer with a dimension of 128 was successfully implemented using PyTorch.\n",
        "*   The implementation of the `PositionalEncoding` class was attempted but failed to correctly add positional information to the input tensor as verified by value comparison.\n",
        "*   The `MultiHeadAttention` mechanism was successfully implemented with 8 heads and an embedding dimension of 128.\n",
        "*   A `FeedForwardNetwork` with an inner dimension 4 times the embedding dimension was successfully implemented.\n",
        "*   A `TransformerBlock` combining multi-head attention, feed-forward network, layer normalization, and dropout was successfully implemented.\n",
        "*   The full `GPT` model architecture, stacking 6 transformer blocks, was successfully assembled and instantiated.\n",
        "*   A dummy text dataset was created, tokenized, and prepared into input-target sequences of length 100.\n",
        "*   A PyTorch `DataLoader` was created to batch the training data with a batch size of 4.\n",
        "*   The model was trained for 5 epochs using the Adam optimizer and Cross-Entropy Loss on the CPU.\n",
        "*   The average training loss decreased from 1.9686 in epoch 1 to 0.1549 in epoch 5, indicating learning.\n",
        "*   The model's state dictionary was saved after training.\n",
        "*   The evaluation loss on the training dataset was calculated as 0.0838, suggesting good performance on the seen data.\n",
        "*   A text generation function was successfully implemented to generate text token by token using the trained model and a starting prompt.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The positional encoding implementation needs to be reviewed and corrected to ensure it is correctly added to the input embeddings.\n",
        "*   To properly evaluate the model's generalization ability and check for overfitting, a separate validation or test dataset should be prepared and used for evaluation.\n"
      ]
    }
  ]
}